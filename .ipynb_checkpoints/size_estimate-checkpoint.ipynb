{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96beaf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (2.31.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (4.12.2)\r\n",
      "Requirement already satisfied: numpy in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (1.24.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (from requests) (2023.11.17)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/hasaniqbal/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf6b4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de451f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the main sitemap file\n",
    "main_sitemap_path = 'sitemap.xml'\n",
    "tree = ET.parse(main_sitemap_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Namespace for parsing sitemap XML\n",
    "namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34bcf944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English URLs: 251,372\n",
      "Total French URLs: 252,305\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters and lists\n",
    "total_english_url_count = 0\n",
    "total_french_url_count = 0\n",
    "english_urls = []\n",
    "french_urls = []\n",
    "\n",
    "# Iterate through each sitemap in the main sitemap\n",
    "for sitemap in root.findall('ns:sitemap', namespace):\n",
    "    sitemap_url = sitemap.find('ns:loc', namespace).text\n",
    "    response = requests.get(sitemap_url)\n",
    "\n",
    "    # Parse the individual sitemap\n",
    "    sitemap_tree = ET.fromstring(response.content)\n",
    "    \n",
    "    # Iterate over each URL in the sitemap\n",
    "    for url_element in sitemap_tree.findall('ns:url', namespace):\n",
    "        url = url_element.find('ns:loc', namespace).text\n",
    "\n",
    "        # Check if English or French\n",
    "        if \"/en/\" in url:\n",
    "            total_english_url_count += 1\n",
    "            english_urls.append(url)\n",
    "        \n",
    "        elif \"/fr/\" in url:\n",
    "            total_french_url_count += 1\n",
    "            french_urls.append(url)\n",
    "\n",
    "print(f\"Total English URLs: {total_english_url_count:,}\")\n",
    "print(f\"Total French URLs: {total_french_url_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3933e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english_urls.txt', 'w') as file:\n",
    "    for url in english_urls:\n",
    "        file.write(url + '\\n')\n",
    "\n",
    "with open('french_urls.txt', 'w') as file:\n",
    "    for url in french_urls:\n",
    "        file.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3cdea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Estimated size = 2.07 GB\n",
      "Iteration 2: Estimated size = 3.64 GB\n",
      "Iteration 3: Estimated size = 2.98 GB\n",
      "Iteration 4: Estimated size = 3.48 GB\n",
      "Iteration 5: Estimated size = 3.36 GB\n",
      "Iteration 6: Estimated size = 1.90 GB\n",
      "Iteration 7: Estimated size = 2.91 GB\n",
      "Iteration 8: Estimated size = 2.62 GB\n",
      "Iteration 9: Estimated size = 2.06 GB\n",
      "Iteration 10: Estimated size = 4.24 GB\n",
      "\n",
      "Mean Estimated Size: 2.93 GB\n",
      "Standard Deviation: 0.73 GB\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "iterations = 10\n",
    "estimates = []\n",
    "\n",
    "# Function to perform one sampling and size estimation\n",
    "def estimate_size(sample_size, urls, total_url_count):\n",
    "    sampled_urls = random.sample(urls, sample_size)\n",
    "    total_sample_size_bytes = 0\n",
    "\n",
    "    for url in sampled_urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text_content = soup.get_text()\n",
    "        page_size_bytes = len(text_content.encode('utf-8'))\n",
    "        total_sample_size_bytes += page_size_bytes\n",
    "\n",
    "    average_page_size_bytes = total_sample_size_bytes / sample_size\n",
    "    estimated_total_size_bytes = average_page_size_bytes * total_url_count\n",
    "    estimated_total_size_gb = estimated_total_size_bytes / (1024 ** 3)\n",
    "    return estimated_total_size_gb\n",
    "\n",
    "for i in range(iterations):\n",
    "    estimate = estimate_size(sample_size, english_urls, total_english_url_count)\n",
    "    estimates.append(estimate)\n",
    "    print(f\"Iteration {i+1}: Estimated size = {estimate:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a40667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Estimated Size (of English pages only): 2.93 GB\n",
      "Standard Deviation: 0.73 GB\n"
     ]
    }
   ],
   "source": [
    "mean_estimate = np.mean(estimates)\n",
    "std_deviation = np.std(estimates)\n",
    "\n",
    "print(f\"\\nMean Estimated Size (of English pages only): {mean_estimate:.2f} GB\")\n",
    "print(f\"Standard Deviation: {std_deviation:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11331015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1,259,466,086.8\n",
      "Total Cost: $503.79\n"
     ]
    }
   ],
   "source": [
    "gb_to_bytes = 1_073_741_824  # Bytes in one gigabyte\n",
    "\n",
    "# https://platform.openai.com/docs/guides/production-best-practices/managing-costs\n",
    "words_to_tokens_ratio = 1000 / 750  # 1,000 tokens per 750 words\n",
    "\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "cost_per_thousand_tokens = 0.0004  # Cost per 1,000 tokens\n",
    "\n",
    "# https://foolip.org/2008/05/17/how-much-a-thousand-words-are-worth/\n",
    "average_bytes_per_word = 4.158\n",
    "\n",
    "# Taking a conservative estimate: mean + 1 std, covering 84% of the data\n",
    "size_in_gb = mean_estimate + std_deviation\n",
    "\n",
    "size_in_bytes = size_in_gb * gb_to_bytes\n",
    "total_words = size_in_bytes / average_bytes_per_word\n",
    "\n",
    "total_tokens = total_words * words_to_tokens_ratio\n",
    "\n",
    "total_cost = (total_tokens / 1000) * cost_per_thousand_tokens\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,.1f}\")\n",
    "print(f\"Total Cost: ${total_cost:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7808a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
